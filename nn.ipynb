{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit_learn==1.2.2 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 1)) (1.2.2)\n",
      "Requirement already satisfied: matplotlib==3.5.1 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from -r requirements.txt (line 2)) (3.5.1)\n",
      "Requirement already satisfied: numpy==1.22 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from -r requirements.txt (line 3)) (1.22.0)\n",
      "Requirement already satisfied: pandas==1.4.2 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from -r requirements.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: thefuzz~=0.19.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 6)) (0.19.0)\n",
      "Requirement already satisfied: ipython~=8.6.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 7)) (8.6.0)\n",
      "Requirement already satisfied: requests~=2.28.2 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 8)) (2.28.2)\n",
      "Requirement already satisfied: seaborn~=0.12.2 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 9)) (0.12.2)\n",
      "Requirement already satisfied: tensorflow==2.12.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 10)) (2.12.0)\n",
      "Requirement already satisfied: plotly==5.14.1 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from -r requirements.txt (line 11)) (5.14.1)\n",
      "Requirement already satisfied: python-utils==3.5.2 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 12)) (3.5.2)\n",
      "Requirement already satisfied: pydot==1.4.2 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 13)) (1.4.2)\n",
      "Requirement already satisfied: graphviz==0.20.1 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from -r requirements.txt (line 14)) (0.20.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from scikit_learn==1.2.2->-r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from scikit_learn==1.2.2->-r requirements.txt (line 1)) (1.9.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from scikit_learn==1.2.2->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib==3.5.1->-r requirements.txt (line 2)) (4.38.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib==3.5.1->-r requirements.txt (line 2)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib==3.5.1->-r requirements.txt (line 2)) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib==3.5.1->-r requirements.txt (line 2)) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib==3.5.1->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib==3.5.1->-r requirements.txt (line 2)) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib==3.5.1->-r requirements.txt (line 2)) (9.4.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from pandas==1.4.2->-r requirements.txt (line 4)) (2022.7.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow==2.12.0->-r requirements.txt (line 10)) (2.12.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from plotly==5.14.1->-r requirements.txt (line 11)) (8.2.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (3.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (2.12.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (3.8.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (58.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (4.23.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (2.0.1)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (2.12.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (22.10.26)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (1.54.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (0.27.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (1.3.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (0.4.10)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (1.16.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (14.0.6)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (1.6.3)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (1.14.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (0.2.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython~=8.6.0->-r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython~=8.6.0->-r requirements.txt (line 7)) (3.0.31)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython~=8.6.0->-r requirements.txt (line 7)) (2.13.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython~=8.6.0->-r requirements.txt (line 7)) (0.1.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython~=8.6.0->-r requirements.txt (line 7)) (5.1.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython~=8.6.0->-r requirements.txt (line 7)) (0.6.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython~=8.6.0->-r requirements.txt (line 7)) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython~=8.6.0->-r requirements.txt (line 7)) (0.18.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython~=8.6.0->-r requirements.txt (line 7)) (0.4.6)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython~=8.6.0->-r requirements.txt (line 7)) (5.9.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests~=2.28.2->-r requirements.txt (line 8)) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests~=2.28.2->-r requirements.txt (line 8)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests~=2.28.2->-r requirements.txt (line 8)) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests~=2.28.2->-r requirements.txt (line 8)) (1.26.15)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jedi>=0.16->ipython~=8.6.0->-r requirements.txt (line 7)) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython~=8.6.0->-r requirements.txt (line 7)) (0.2.5)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stack-data->ipython~=8.6.0->-r requirements.txt (line 7)) (2.1.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stack-data->ipython~=8.6.0->-r requirements.txt (line 7)) (0.2.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stack-data->ipython~=8.6.0->-r requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (0.40.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (0.1.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (2.18.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (2.3.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (3.4.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (0.7.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (6.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (3.12.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.1.2\n",
      "[notice] To update, run: C:\\Users\\mosas\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from math import log2\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.utils.vis_utils import plot_model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def top_3(y_true, y_pred):\n",
    "    true = 0\n",
    "    length = len(y_pred)\n",
    "    if length == 0:\n",
    "        return 0\n",
    "    for i in range(len(y_pred)):\n",
    "        for j in range(len(y_pred[i])):\n",
    "            if j==3:\n",
    "                continue\n",
    "            if y_pred[i][j] == y_true[i]:\n",
    "                true+=1\n",
    "    return true/length"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def nDCG(y_true, y_pred):\n",
    "    relevance = []\n",
    "    length = len(y_pred)\n",
    "    if length == 0:\n",
    "        return 0\n",
    "    for i in range(len(y_pred)):\n",
    "        relevance.append([])\n",
    "        for j in range(3):\n",
    "            if j >= len(y_pred[i]):\n",
    "                relevance[i].append(0)\n",
    "            elif y_pred[i][j] == y_true[i]:\n",
    "                relevance[i].append(1)\n",
    "            else:\n",
    "                relevance[i].append(0)\n",
    "    ndcg = 0\n",
    "    for element in relevance:\n",
    "        ideal = element.copy()\n",
    "        ideal.sort(reverse=True)\n",
    "        dcg = 0\n",
    "        idcg = 0\n",
    "        for i in range(len(element)):\n",
    "            dcg += element[i]/log2(i+2)\n",
    "            idcg += ideal[i]/log2(i+2)\n",
    "        if idcg != 0:\n",
    "            ndcg += dcg/idcg\n",
    "    ndcg /= len(relevance)\n",
    "    return ndcg"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "size = [\"small\",\"medium\",\"big\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 70, 200)           14200     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 70, 200)           320800    \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 200)               320800    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 200)               40200     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 71)                14271     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 710,271\n",
      "Trainable params: 710,271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "1/2 [==============>...............] - ETA: 4s - loss: 4.2627\n",
      "Epoch 1: loss improved from inf to 4.25850, saving model to small.h5\n",
      "2/2 [==============================] - 4s 54ms/step - loss: 4.2585 - lr: 0.0100\n",
      "Epoch 2/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.1700\n",
      "Epoch 2: loss improved from 4.25850 to 4.14467, saving model to small.h5\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 4.1447 - lr: 0.0100\n",
      "Epoch 3/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.8786\n",
      "Epoch 3: loss improved from 4.14467 to 3.68672, saving model to small.h5\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 3.6867 - lr: 0.0100\n",
      "Epoch 4/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.1430\n",
      "Epoch 4: loss improved from 3.68672 to 3.03116, saving model to small.h5\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 3.0312 - lr: 0.0100\n",
      "Epoch 5/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5414\n",
      "Epoch 5: loss improved from 3.03116 to 2.75381, saving model to small.h5\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 2.7538 - lr: 0.0100\n",
      "Epoch 6/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8640\n",
      "Epoch 6: loss did not improve from 2.75381\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 2.7666 - lr: 0.0100\n",
      "Epoch 7/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5544\n",
      "Epoch 7: loss improved from 2.75381 to 2.40897, saving model to small.h5\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 2.4090 - lr: 0.0100\n",
      "Epoch 8/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.1325\n",
      "Epoch 8: loss improved from 2.40897 to 2.13040, saving model to small.h5\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 2.1304 - lr: 0.0100\n",
      "Epoch 9/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.6531\n",
      "Epoch 9: loss improved from 2.13040 to 1.73384, saving model to small.h5\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.7338 - lr: 0.0100\n",
      "Epoch 10/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.4095\n",
      "Epoch 10: loss improved from 1.73384 to 1.49242, saving model to small.h5\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1.4924 - lr: 0.0100\n",
      "Epoch 11/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.3951\n",
      "Epoch 11: loss improved from 1.49242 to 1.22966, saving model to small.h5\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.2297 - lr: 0.0100\n",
      "Epoch 12/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.0189\n",
      "Epoch 12: loss improved from 1.22966 to 1.00220, saving model to small.h5\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 1.0022 - lr: 0.0100\n",
      "Epoch 13/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6785\n",
      "Epoch 13: loss improved from 1.00220 to 0.73292, saving model to small.h5\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.7329 - lr: 0.0100\n",
      "Epoch 14/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6200\n",
      "Epoch 14: loss improved from 0.73292 to 0.54665, saving model to small.h5\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5467 - lr: 0.0100\n",
      "Epoch 15/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5406\n",
      "Epoch 15: loss improved from 0.54665 to 0.42205, saving model to small.h5\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4220 - lr: 0.0100\n",
      "Epoch 16/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1976\n",
      "Epoch 16: loss improved from 0.42205 to 0.36706, saving model to small.h5\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3671 - lr: 0.0100\n",
      "Epoch 17/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1837\n",
      "Epoch 17: loss improved from 0.36706 to 0.20570, saving model to small.h5\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.2057 - lr: 0.0100\n",
      "Epoch 18/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1450\n",
      "Epoch 18: loss did not improve from 0.20570\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.2997 - lr: 0.0100\n",
      "Epoch 19/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1575\n",
      "Epoch 19: loss improved from 0.20570 to 0.14910, saving model to small.h5\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.1491 - lr: 0.0100\n",
      "Epoch 20/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1669\n",
      "Epoch 20: loss did not improve from 0.14910\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1741 - lr: 0.0100\n",
      "Epoch 21/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2096\n",
      "Epoch 21: loss did not improve from 0.14910\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1776 - lr: 0.0100\n",
      "Epoch 22/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1817\n",
      "Epoch 22: loss improved from 0.14910 to 0.12309, saving model to small.h5\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.1231 - lr: 0.0100\n",
      "Epoch 23/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1641\n",
      "Epoch 23: loss did not improve from 0.12309\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1686 - lr: 0.0100\n",
      "Epoch 24/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2003\n",
      "Epoch 24: loss did not improve from 0.12309\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1396 - lr: 0.0100\n",
      "Epoch 25/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0613\n",
      "Epoch 25: loss improved from 0.12309 to 0.10598, saving model to small.h5\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.1060 - lr: 0.0100\n",
      "Epoch 26/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1054\n",
      "Epoch 26: loss did not improve from 0.10598\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1344 - lr: 0.0100\n",
      "Epoch 27/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1121\n",
      "Epoch 27: loss did not improve from 0.10598\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1329 - lr: 0.0100\n",
      "Epoch 28/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0784\n",
      "Epoch 28: loss improved from 0.10598 to 0.10107, saving model to small.h5\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.1011 - lr: 0.0100\n",
      "Epoch 29/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0751\n",
      "Epoch 29: loss did not improve from 0.10107\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1276 - lr: 0.0100\n",
      "Epoch 30/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0854\n",
      "Epoch 30: loss did not improve from 0.10107\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1082 - lr: 0.0100\n",
      "Epoch 31/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1116\n",
      "Epoch 31: loss improved from 0.10107 to 0.09460, saving model to small.h5\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.0946 - lr: 0.0100\n",
      "Epoch 32/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1058\n",
      "Epoch 32: loss did not improve from 0.09460\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1124 - lr: 0.0100\n",
      "Epoch 33/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0668\n",
      "Epoch 33: loss did not improve from 0.09460\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1244 - lr: 0.0100\n",
      "Epoch 34/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1730\n",
      "Epoch 34: loss did not improve from 0.09460\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1243 - lr: 0.0100\n",
      "Epoch 35/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1311\n",
      "Epoch 35: loss did not improve from 0.09460\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1121 - lr: 0.0020\n",
      "Epoch 36/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0613\n",
      "Epoch 36: loss did not improve from 0.09460\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1068 - lr: 0.0020\n",
      "Epoch 37/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1306\n",
      "Epoch 37: loss did not improve from 0.09460\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1022 - lr: 0.0020\n",
      "Epoch 38/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0815\n",
      "Epoch 38: loss improved from 0.09460 to 0.09378, saving model to small.h5\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.0938 - lr: 4.0000e-04\n",
      "Epoch 39/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1101\n",
      "Epoch 39: loss did not improve from 0.09378\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0940 - lr: 4.0000e-04\n",
      "Epoch 40/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0941\n",
      "Epoch 40: loss improved from 0.09378 to 0.09224, saving model to small.h5\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.0922 - lr: 4.0000e-04\n",
      "Epoch 41/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0288\n",
      "Epoch 41: loss improved from 0.09224 to 0.09173, saving model to small.h5\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.0917 - lr: 4.0000e-04\n",
      "Epoch 42/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1150\n",
      "Epoch 42: loss improved from 0.09173 to 0.09113, saving model to small.h5\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0911 - lr: 4.0000e-04\n",
      "Epoch 43/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1353\n",
      "Epoch 43: loss improved from 0.09113 to 0.09071, saving model to small.h5\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.0907 - lr: 4.0000e-04\n",
      "Epoch 44/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1073\n",
      "Epoch 44: loss improved from 0.09071 to 0.09014, saving model to small.h5\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.0901 - lr: 4.0000e-04\n",
      "Epoch 45/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0531\n",
      "Epoch 45: loss did not improve from 0.09014\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0909 - lr: 4.0000e-04\n",
      "Epoch 46/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1155\n",
      "Epoch 46: loss improved from 0.09014 to 0.09008, saving model to small.h5\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0901 - lr: 4.0000e-04\n",
      "Epoch 47/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0886\n",
      "Epoch 47: loss did not improve from 0.09008\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0904 - lr: 4.0000e-04\n",
      "Epoch 48/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0207\n",
      "Epoch 48: loss improved from 0.09008 to 0.08983, saving model to small.h5\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0898 - lr: 1.0000e-04\n",
      "Epoch 49/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0899\n",
      "Epoch 49: loss improved from 0.08983 to 0.08978, saving model to small.h5\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0898 - lr: 1.0000e-04\n",
      "Epoch 50/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0651\n",
      "Epoch 50: loss improved from 0.08978 to 0.08977, saving model to small.h5\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0898 - lr: 1.0000e-04\n",
      "Epoch 51/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0667\n",
      "Epoch 51: loss did not improve from 0.08977\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0898 - lr: 1.0000e-04\n",
      "Epoch 52/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0663\n",
      "Epoch 52: loss improved from 0.08977 to 0.08967, saving model to small.h5\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0897 - lr: 1.0000e-04\n",
      "Epoch 53/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1126\n",
      "Epoch 53: loss did not improve from 0.08967\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0897 - lr: 1.0000e-04\n",
      "Epoch 54/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0879\n",
      "Epoch 54: loss improved from 0.08967 to 0.08966, saving model to small.h5\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0897 - lr: 1.0000e-04\n",
      "Epoch 55/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0859\n",
      "Epoch 55: loss improved from 0.08966 to 0.08959, saving model to small.h5\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0896 - lr: 1.0000e-04\n",
      "Epoch 56/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0445\n",
      "Epoch 56: loss improved from 0.08959 to 0.08952, saving model to small.h5\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0895 - lr: 1.0000e-04\n",
      "Epoch 57/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1132\n",
      "Epoch 57: loss did not improve from 0.08952\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0895 - lr: 1.0000e-04\n",
      "Epoch 58/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1130\n",
      "Epoch 58: loss improved from 0.08952 to 0.08947, saving model to small.h5\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0895 - lr: 1.0000e-04\n",
      "Epoch 59/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0892\n",
      "Epoch 59: loss did not improve from 0.08947\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0896 - lr: 1.0000e-04\n",
      "Epoch 60/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1098\n",
      "Epoch 60: loss improved from 0.08947 to 0.08943, saving model to small.h5\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0894 - lr: 1.0000e-04\n",
      "Epoch 61/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1335\n",
      "Epoch 61: loss improved from 0.08943 to 0.08937, saving model to small.h5\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0894 - lr: 1.0000e-04\n",
      "Epoch 62/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0660\n",
      "Epoch 62: loss improved from 0.08937 to 0.08936, saving model to small.h5\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0894 - lr: 1.0000e-04\n",
      "Epoch 63/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1095\n",
      "Epoch 63: loss improved from 0.08936 to 0.08935, saving model to small.h5\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0893 - lr: 1.0000e-04\n",
      "Epoch 64/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1107\n",
      "Epoch 64: loss improved from 0.08935 to 0.08932, saving model to small.h5\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0893 - lr: 1.0000e-04\n",
      "Epoch 65/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0433\n",
      "Epoch 65: loss improved from 0.08932 to 0.08926, saving model to small.h5\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0893 - lr: 1.0000e-04\n",
      "Epoch 66/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1120\n",
      "Epoch 66: loss did not improve from 0.08926\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0894 - lr: 1.0000e-04\n",
      "Epoch 67/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0873\n",
      "Epoch 67: loss improved from 0.08926 to 0.08924, saving model to small.h5\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0892 - lr: 1.0000e-04\n",
      "Epoch 68/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0672\n",
      "Epoch 68: loss did not improve from 0.08924\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0893 - lr: 1.0000e-04\n",
      "Epoch 69/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0684\n",
      "Epoch 69: loss did not improve from 0.08924\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0893 - lr: 1.0000e-04\n",
      "Epoch 70/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0892\n",
      "Epoch 70: loss did not improve from 0.08924\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0893 - lr: 1.0000e-04\n",
      "Epoch 71/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0894\n",
      "Epoch 71: loss improved from 0.08924 to 0.08918, saving model to small.h5\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0892 - lr: 1.0000e-04\n",
      "Epoch 72/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1107\n",
      "Epoch 72: loss improved from 0.08918 to 0.08912, saving model to small.h5\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0891 - lr: 1.0000e-04\n",
      "Epoch 73/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0890\n",
      "Epoch 73: loss did not improve from 0.08912\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0893 - lr: 1.0000e-04\n",
      "Epoch 74/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1333\n",
      "Epoch 74: loss did not improve from 0.08912\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0891 - lr: 1.0000e-04\n",
      "Epoch 75/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0672\n",
      "Epoch 75: loss did not improve from 0.08912\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0891 - lr: 1.0000e-04\n",
      "Epoch 76/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0902\n",
      "Epoch 76: loss improved from 0.08912 to 0.08910, saving model to small.h5\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0891 - lr: 1.0000e-04\n",
      "Epoch 77/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1112\n",
      "Epoch 77: loss improved from 0.08910 to 0.08910, saving model to small.h5\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0891 - lr: 1.0000e-04\n",
      "Epoch 78/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0905\n",
      "Epoch 78: loss improved from 0.08910 to 0.08905, saving model to small.h5\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0891 - lr: 1.0000e-04\n",
      "Epoch 79/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0894\n",
      "Epoch 79: loss did not improve from 0.08905\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0891 - lr: 1.0000e-04\n",
      "Epoch 80/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0668\n",
      "Epoch 80: loss did not improve from 0.08905\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0891 - lr: 1.0000e-04\n",
      "Epoch 81/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0671\n",
      "Epoch 81: loss did not improve from 0.08905\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0891 - lr: 1.0000e-04\n",
      "Epoch 82/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1330\n",
      "Epoch 82: loss improved from 0.08905 to 0.08904, saving model to small.h5\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0890 - lr: 1.0000e-04\n",
      "Epoch 83/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0894\n",
      "Epoch 83: loss did not improve from 0.08904\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0891 - lr: 1.0000e-04\n",
      "Epoch 84/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1120\n",
      "Epoch 84: loss did not improve from 0.08904\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0891 - lr: 1.0000e-04\n",
      "Epoch 85/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0656\n",
      "Epoch 85: loss did not improve from 0.08904\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0890 - lr: 1.0000e-04\n",
      "Epoch 86/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0224\n",
      "Epoch 86: loss improved from 0.08904 to 0.08901, saving model to small.h5\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0890 - lr: 1.0000e-04\n",
      "Epoch 87/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0654\n",
      "Epoch 87: loss did not improve from 0.08901\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0891 - lr: 1.0000e-04\n",
      "Epoch 88/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0454\n",
      "Epoch 88: loss improved from 0.08901 to 0.08896, saving model to small.h5\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0890 - lr: 1.0000e-04\n",
      "Epoch 89/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0685\n",
      "Epoch 89: loss did not improve from 0.08896\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0890 - lr: 1.0000e-04\n",
      "Epoch 90/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1119\n",
      "Epoch 90: loss did not improve from 0.08896\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0890 - lr: 1.0000e-04\n",
      "Epoch 91/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0650\n",
      "Epoch 91: loss did not improve from 0.08896\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0890 - lr: 1.0000e-04\n",
      "Epoch 92/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0891\n",
      "Epoch 92: loss improved from 0.08896 to 0.08891, saving model to small.h5\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0889 - lr: 1.0000e-04\n",
      "Epoch 93/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1328\n",
      "Epoch 93: loss improved from 0.08891 to 0.08890, saving model to small.h5\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0889 - lr: 1.0000e-04\n",
      "Epoch 94/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0862\n",
      "Epoch 94: loss did not improve from 0.08890\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0889 - lr: 1.0000e-04\n",
      "Epoch 95/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0872\n",
      "Epoch 95: loss improved from 0.08890 to 0.08886, saving model to small.h5\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0889 - lr: 1.0000e-04\n",
      "Epoch 96/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1101\n",
      "Epoch 96: loss improved from 0.08886 to 0.08886, saving model to small.h5\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0889 - lr: 1.0000e-04\n",
      "Epoch 97/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0902\n",
      "Epoch 97: loss did not improve from 0.08886\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0889 - lr: 1.0000e-04\n",
      "Epoch 98/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0658\n",
      "Epoch 98: loss improved from 0.08886 to 0.08880, saving model to small.h5\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0888 - lr: 1.0000e-04\n",
      "Epoch 99/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0667\n",
      "Epoch 99: loss did not improve from 0.08880\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0889 - lr: 1.0000e-04\n",
      "Epoch 100/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0875\n",
      "Epoch 100: loss did not improve from 0.08880\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0889 - lr: 1.0000e-04\n",
      "Epoch 101/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0891\n",
      "Epoch 101: loss did not improve from 0.08880\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0888 - lr: 1.0000e-04\n",
      "Epoch 102/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0908\n",
      "Epoch 102: loss did not improve from 0.08880\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0889 - lr: 1.0000e-04\n",
      "Epoch 103/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0446\n",
      "Epoch 103: loss did not improve from 0.08880\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0889 - lr: 1.0000e-04\n",
      "Epoch 104/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0684\n",
      "Epoch 104: loss did not improve from 0.08880\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0889 - lr: 1.0000e-04\n",
      "Epoch 105/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0449\n",
      "Epoch 105: loss improved from 0.08880 to 0.08876, saving model to small.h5\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0888 - lr: 1.0000e-04\n",
      "Epoch 106/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0880\n",
      "Epoch 106: loss did not improve from 0.08876\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0888 - lr: 1.0000e-04\n",
      "Epoch 107/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0672\n",
      "Epoch 107: loss did not improve from 0.08876\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0889 - lr: 1.0000e-04\n",
      "Epoch 108/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0442\n",
      "Epoch 108: loss improved from 0.08876 to 0.08874, saving model to small.h5\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0887 - lr: 1.0000e-04\n",
      "Epoch 109/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0889\n",
      "Epoch 109: loss did not improve from 0.08874\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0887 - lr: 1.0000e-04\n",
      "Epoch 110/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0664\n",
      "Epoch 110: loss did not improve from 0.08874\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0889 - lr: 1.0000e-04\n",
      "Epoch 111/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0673\n",
      "Epoch 111: loss did not improve from 0.08874\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0888 - lr: 1.0000e-04\n",
      "Epoch 112/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0873\n",
      "Epoch 112: loss did not improve from 0.08874\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0888 - lr: 1.0000e-04\n",
      "Epoch 113/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1114\n",
      "Epoch 113: loss did not improve from 0.08874\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0888 - lr: 1.0000e-04\n",
      "Epoch 114/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0890\n",
      "Epoch 114: loss improved from 0.08874 to 0.08872, saving model to small.h5\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0887 - lr: 1.0000e-04\n",
      "Epoch 115/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0663\n",
      "Epoch 115: loss did not improve from 0.08872\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0888 - lr: 1.0000e-04\n",
      "Epoch 116/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0896\n",
      "Epoch 116: loss improved from 0.08872 to 0.08870, saving model to small.h5\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.0887 - lr: 1.0000e-04\n",
      "Epoch 117/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0436\n",
      "Epoch 117: loss improved from 0.08870 to 0.08869, saving model to small.h5\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.0887 - lr: 1.0000e-04\n",
      "Epoch 118/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1100\n",
      "Epoch 118: loss improved from 0.08869 to 0.08866, saving model to small.h5\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.0887 - lr: 1.0000e-04\n",
      "Epoch 119/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0888\n",
      "Epoch 119: loss improved from 0.08866 to 0.08863, saving model to small.h5\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.0886 - lr: 1.0000e-04\n",
      "Epoch 120/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0882\n",
      "Epoch 120: loss improved from 0.08863 to 0.08861, saving model to small.h5\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.0886 - lr: 1.0000e-04\n",
      "Epoch 121/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1099\n",
      "Epoch 121: loss improved from 0.08861 to 0.08859, saving model to small.h5\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.0886 - lr: 1.0000e-04\n",
      "Epoch 122/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1114\n",
      "Epoch 122: loss improved from 0.08859 to 0.08855, saving model to small.h5\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.0886 - lr: 1.0000e-04\n",
      "Epoch 123/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1100\n",
      "Epoch 123: loss did not improve from 0.08855\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0887 - lr: 1.0000e-04\n",
      "Epoch 124/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0894\n",
      "Epoch 124: loss did not improve from 0.08855\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0886 - lr: 1.0000e-04\n",
      "Epoch 125/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1093\n",
      "Epoch 125: loss did not improve from 0.08855\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0886 - lr: 1.0000e-04\n",
      "Epoch 126/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1323\n",
      "Epoch 126: loss did not improve from 0.08855\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0886 - lr: 1.0000e-04\n",
      "Epoch 127/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0656\n",
      "Epoch 127: loss did not improve from 0.08855\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0886 - lr: 1.0000e-04\n",
      "Epoch 128/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0648\n",
      "Epoch 128: loss did not improve from 0.08855\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0886 - lr: 1.0000e-04\n",
      "Epoch 129/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0909\n",
      "Epoch 129: loss did not improve from 0.08855\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0887 - lr: 1.0000e-04\n",
      "Epoch 130/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1095\n",
      "Epoch 130: loss did not improve from 0.08855\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0886 - lr: 1.0000e-04\n",
      "Epoch 131/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1321\n",
      "Epoch 131: loss improved from 0.08855 to 0.08850, saving model to small.h5\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.0885 - lr: 1.0000e-04\n",
      "Epoch 132/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0883\n",
      "Epoch 132: loss did not improve from 0.08850\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0885 - lr: 1.0000e-04\n",
      "Epoch 133/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1090\n",
      "Epoch 133: loss did not improve from 0.08850\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0885 - lr: 1.0000e-04\n",
      "Epoch 134/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1115\n",
      "Epoch 134: loss did not improve from 0.08850\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0885 - lr: 1.0000e-04\n",
      "Epoch 135/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0881\n",
      "Epoch 135: loss did not improve from 0.08850\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0886 - lr: 1.0000e-04\n",
      "Epoch 136/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1096\n",
      "Epoch 136: loss did not improve from 0.08850\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0885 - lr: 1.0000e-04\n",
      "Epoch 137/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0665\n",
      "Epoch 137: loss did not improve from 0.08850\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0885 - lr: 1.0000e-04\n",
      "Epoch 138/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0885\n",
      "Epoch 138: loss improved from 0.08850 to 0.08847, saving model to small.h5\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.0885 - lr: 1.0000e-04\n",
      "Epoch 139/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0663\n",
      "Epoch 139: loss did not improve from 0.08847\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0887 - lr: 1.0000e-04\n",
      "Epoch 140/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1113\n",
      "Epoch 140: loss did not improve from 0.08847\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0885 - lr: 1.0000e-04\n",
      "Epoch 141/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0444\n",
      "Epoch 141: loss improved from 0.08847 to 0.08843, saving model to small.h5\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.0884 - lr: 1.0000e-04\n",
      "Epoch 142/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1101\n",
      "Epoch 142: loss did not improve from 0.08843\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0884 - lr: 1.0000e-04\n",
      "Epoch 143/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0897\n",
      "Epoch 143: loss did not improve from 0.08843\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0884 - lr: 1.0000e-04\n",
      "Epoch 144/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1107\n",
      "Epoch 144: loss improved from 0.08843 to 0.08838, saving model to small.h5\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.0884 - lr: 1.0000e-04\n",
      "Epoch 145/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0887\n",
      "Epoch 145: loss did not improve from 0.08838\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0884 - lr: 1.0000e-04\n",
      "Epoch 146/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0901\n",
      "Epoch 146: loss did not improve from 0.08838\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0884 - lr: 1.0000e-04\n",
      "Epoch 147/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0865\n",
      "Epoch 147: loss did not improve from 0.08838\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0886 - lr: 1.0000e-04\n",
      "Epoch 148/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1099\n",
      "Epoch 148: loss did not improve from 0.08838\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0885 - lr: 1.0000e-04\n",
      "Epoch 149/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0894\n",
      "Epoch 149: loss did not improve from 0.08838\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0885 - lr: 1.0000e-04\n",
      "Epoch 150/150\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0913\n",
      "Epoch 150: loss did not improve from 0.08838\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0885 - lr: 1.0000e-04\n",
      "top 1 accuracy: 0.24\n",
      "top 3 accuracy: 0.48\n",
      "ndcg: 0.38618595071429157\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name in size:\n",
    "    data = pd.read_csv(f'code_completion_lib/methods/models/small/data.csv')\n",
    "    X = data[[\"varible_name\"]]\n",
    "    y = data[[\"method\"]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    s = pd.Series(data=X_train[\"varible_name\"]+\" \" + y_train[\"method\"])\n",
    "\n",
    "\n",
    "    tokenizer = Tokenizer(oov_token='<oov>',split=\" \", filters='!',lower=False) # For those words which are not found in word_index\n",
    "    tokenizer.fit_on_texts(s)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    #print(\"Total number of words: \", total_words)\n",
    "\n",
    "\n",
    "    input_sequences = []\n",
    "    for line in s:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        input_sequences.append(token_list)\n",
    "\n",
    "    #print(input_sequences)\n",
    "    #print(\"Total input sequences: \", len(input_sequences))\n",
    "    # pad sequences\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "    # create features and label\n",
    "    X = input_sequences[:,:-1]\n",
    "    labels = input_sequences[:,-1]\n",
    "    y = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
    "\n",
    "    # saving the tokenizer for predict function.\n",
    "    pickle.dump(tokenizer, open(f'{name}.pkl', 'wb'))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 200, input_length=total_words - 1))\n",
    "    model.add(LSTM(200, return_sequences=True))\n",
    "    model.add(LSTM(200))\n",
    "    model.add(Dense(200, activation=\"relu\"))\n",
    "    model.add(Dense(total_words, activation=\"softmax\"))\n",
    "    print(model.summary())\n",
    "\n",
    "    checkpoint = ModelCheckpoint(f\"{name}.h5\", monitor='loss', verbose=1,\n",
    "                                 save_best_only=True, mode='auto')\n",
    "\n",
    "    reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "    logdir='logsnextword1'\n",
    "    tensorboard_Visualization = TensorBoard(log_dir=logdir)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.01))\n",
    "\n",
    "    model.fit(X, y, epochs=150, callbacks=[checkpoint, reduce, tensorboard_Visualization])\n",
    "\n",
    "\n",
    "    model = load_model(f'{name}.h5')\n",
    "    tokenizer = pickle.load(open(f'{name}.pkl', 'rb'))\n",
    "    y_pred_top1 = []\n",
    "    y_pred_top3 = []\n",
    "    y_true = y_test['method'].values.tolist()\n",
    "    x_for_pred = X_test['varible_name'].values.tolist()\n",
    "    for i in range(X_test.shape[0]):\n",
    "        token_list = tokenizer.texts_to_sequences([x_for_pred[i]])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predict_x=model.predict(token_list,verbose=0)\n",
    "        f = predict_x[0].copy()\n",
    "        f.sort()\n",
    "        a = np.flip(f)\n",
    "        max_index = []\n",
    "        for i in range(len(predict_x[0])):\n",
    "            if predict_x[0][i]== a[0] or predict_x[0][i]== a[1] or predict_x[0][i]== a[2]:\n",
    "                max_index.append(i)\n",
    "        classes_x=np.argmax(predict_x,axis=1)\n",
    "        max_index = max_index[:3]\n",
    "        result = []\n",
    "        for j in range(3):\n",
    "            for word, index in tokenizer.word_index.items():\n",
    "                if index == max_index[j]:\n",
    "                    result.append(word)\n",
    "                    break\n",
    "        y_pred_top1.append(result[0])\n",
    "        y_pred_top3.append(result)\n",
    "\n",
    "    acc_top1 = accuracy_score(y_true, y_pred_top1)\n",
    "    acc_top3 = top_3(y_true, y_pred_top3)\n",
    "    ndcg = nDCG(y_true, y_pred_top3)\n",
    "\n",
    "    print(f\"top 1 accuracy: {acc_top1}\")\n",
    "    print(f\"top 3 accuracy: {acc_top3}\")\n",
    "    print(f\"ndcg: {ndcg}\\n\")\n",
    "    print(\"-----------------------------------------------------------------------------------------------------------\")\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
