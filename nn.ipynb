{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit_learn==1.2.2 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 1)) (1.2.2)\n",
      "Requirement already satisfied: matplotlib==3.5.1 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from -r requirements.txt (line 2)) (3.5.1)\n",
      "Requirement already satisfied: numpy==1.22 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from -r requirements.txt (line 3)) (1.22.0)\n",
      "Requirement already satisfied: pandas==1.4.2 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from -r requirements.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: thefuzz~=0.19.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 6)) (0.19.0)\n",
      "Requirement already satisfied: ipython~=8.6.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 7)) (8.6.0)\n",
      "Requirement already satisfied: requests~=2.28.2 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 8)) (2.28.2)\n",
      "Requirement already satisfied: seaborn~=0.12.2 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 9)) (0.12.2)\n",
      "Requirement already satisfied: tensorflow==2.12.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 10)) (2.12.0)\n",
      "Requirement already satisfied: plotly==5.14.1 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from -r requirements.txt (line 11)) (5.14.1)\n",
      "Requirement already satisfied: python-utils==3.5.2 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 12)) (3.5.2)\n",
      "Requirement already satisfied: pydot==1.4.2 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r requirements.txt (line 13)) (1.4.2)\n",
      "Requirement already satisfied: graphviz==0.20.1 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from -r requirements.txt (line 14)) (0.20.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from scikit_learn==1.2.2->-r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from scikit_learn==1.2.2->-r requirements.txt (line 1)) (1.9.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from scikit_learn==1.2.2->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib==3.5.1->-r requirements.txt (line 2)) (4.38.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib==3.5.1->-r requirements.txt (line 2)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib==3.5.1->-r requirements.txt (line 2)) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib==3.5.1->-r requirements.txt (line 2)) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib==3.5.1->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib==3.5.1->-r requirements.txt (line 2)) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib==3.5.1->-r requirements.txt (line 2)) (9.4.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from pandas==1.4.2->-r requirements.txt (line 4)) (2022.7.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow==2.12.0->-r requirements.txt (line 10)) (2.12.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from plotly==5.14.1->-r requirements.txt (line 11)) (8.2.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (3.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (2.12.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (3.8.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (58.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (4.23.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (2.0.1)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (2.12.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (22.10.26)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (1.54.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (0.27.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (1.3.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (0.4.10)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (1.16.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (14.0.6)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (1.6.3)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (1.14.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (0.2.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython~=8.6.0->-r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython~=8.6.0->-r requirements.txt (line 7)) (3.0.31)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython~=8.6.0->-r requirements.txt (line 7)) (2.13.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython~=8.6.0->-r requirements.txt (line 7)) (0.1.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython~=8.6.0->-r requirements.txt (line 7)) (5.1.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython~=8.6.0->-r requirements.txt (line 7)) (0.6.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython~=8.6.0->-r requirements.txt (line 7)) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython~=8.6.0->-r requirements.txt (line 7)) (0.18.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython~=8.6.0->-r requirements.txt (line 7)) (0.4.6)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython~=8.6.0->-r requirements.txt (line 7)) (5.9.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests~=2.28.2->-r requirements.txt (line 8)) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests~=2.28.2->-r requirements.txt (line 8)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests~=2.28.2->-r requirements.txt (line 8)) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests~=2.28.2->-r requirements.txt (line 8)) (1.26.15)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jedi>=0.16->ipython~=8.6.0->-r requirements.txt (line 7)) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython~=8.6.0->-r requirements.txt (line 7)) (0.2.5)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stack-data->ipython~=8.6.0->-r requirements.txt (line 7)) (2.1.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stack-data->ipython~=8.6.0->-r requirements.txt (line 7)) (0.2.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stack-data->ipython~=8.6.0->-r requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (0.40.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (0.1.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (2.18.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (2.3.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (3.4.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (0.7.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (6.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (3.12.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\mosas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\mosas\\appdata\\roaming\\python\\python39\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 10)) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.1.2\n",
      "[notice] To update, run: C:\\Users\\mosas\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from math import log2\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense,Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.utils.vis_utils import plot_model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)\n",
    "# for later versions:\n",
    "# tf.compat.v1.set_random_seed(seed_value)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def top_3(y_true, y_pred):\n",
    "    true = 0\n",
    "    length = len(y_pred)\n",
    "    if length == 0:\n",
    "        return 0\n",
    "    for i in range(len(y_pred)):\n",
    "        for j in range(len(y_pred[i])):\n",
    "            if j==3:\n",
    "                continue\n",
    "            if y_pred[i][j] == y_true[i]:\n",
    "                true+=1\n",
    "    return true/length"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def nDCG(y_true, y_pred):\n",
    "    relevance = []\n",
    "    length = len(y_pred)\n",
    "    if length == 0:\n",
    "        return 0\n",
    "    for i in range(len(y_pred)):\n",
    "        relevance.append([])\n",
    "        for j in range(3):\n",
    "            if j >= len(y_pred[i]):\n",
    "                relevance[i].append(0)\n",
    "            elif y_pred[i][j] == y_true[i]:\n",
    "                relevance[i].append(1)\n",
    "            else:\n",
    "                relevance[i].append(0)\n",
    "    ndcg = 0\n",
    "    for element in relevance:\n",
    "        ideal = element.copy()\n",
    "        ideal.sort(reverse=True)\n",
    "        dcg = 0\n",
    "        idcg = 0\n",
    "        for i in range(len(element)):\n",
    "            dcg += element[i]/log2(i+2)\n",
    "            idcg += ideal[i]/log2(i+2)\n",
    "        if idcg != 0:\n",
    "            ndcg += dcg/idcg\n",
    "    ndcg /= len(relevance)\n",
    "    return ndcg"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "size = [\"small\",\"medium\",\"big\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size:  small\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     (None, 8, 200)            195000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 1000)             2804000   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 500)               500500    \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 975)               488475    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,987,975\n",
      "Trainable params: 3,987,975\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 5.8959\n",
      "Epoch 1: loss improved from inf to 5.89585, saving model to small.h5\n",
      "46/46 [==============================] - 13s 204ms/step - loss: 5.8959 - lr: 0.0100\n",
      "Epoch 2/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 4.3075\n",
      "Epoch 2: loss improved from 5.89585 to 4.30745, saving model to small.h5\n",
      "46/46 [==============================] - 8s 177ms/step - loss: 4.3075 - lr: 0.0100\n",
      "Epoch 3/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 3.8235\n",
      "Epoch 3: loss improved from 4.30745 to 3.82347, saving model to small.h5\n",
      "46/46 [==============================] - 8s 183ms/step - loss: 3.8235 - lr: 0.0100\n",
      "Epoch 4/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 3.3566\n",
      "Epoch 4: loss improved from 3.82347 to 3.35661, saving model to small.h5\n",
      "46/46 [==============================] - 8s 169ms/step - loss: 3.3566 - lr: 0.0100\n",
      "Epoch 5/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 3.0438\n",
      "Epoch 5: loss improved from 3.35661 to 3.04383, saving model to small.h5\n",
      "46/46 [==============================] - 7s 147ms/step - loss: 3.0438 - lr: 0.0100\n",
      "Epoch 6/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 2.7329\n",
      "Epoch 6: loss improved from 3.04383 to 2.73295, saving model to small.h5\n",
      "46/46 [==============================] - 6s 140ms/step - loss: 2.7329 - lr: 0.0100\n",
      "Epoch 7/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 2.3560\n",
      "Epoch 7: loss improved from 2.73295 to 2.35604, saving model to small.h5\n",
      "46/46 [==============================] - 7s 149ms/step - loss: 2.3560 - lr: 0.0100\n",
      "Epoch 8/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 2.0223\n",
      "Epoch 8: loss improved from 2.35604 to 2.02234, saving model to small.h5\n",
      "46/46 [==============================] - 7s 148ms/step - loss: 2.0223 - lr: 0.0100\n",
      "Epoch 9/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 1.7703\n",
      "Epoch 9: loss improved from 2.02234 to 1.77026, saving model to small.h5\n",
      "46/46 [==============================] - 7s 147ms/step - loss: 1.7703 - lr: 0.0100\n",
      "Epoch 10/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 1.6038\n",
      "Epoch 10: loss improved from 1.77026 to 1.60381, saving model to small.h5\n",
      "46/46 [==============================] - 6s 141ms/step - loss: 1.6038 - lr: 0.0100\n",
      "Epoch 11/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 1.3783\n",
      "Epoch 11: loss improved from 1.60381 to 1.37834, saving model to small.h5\n",
      "46/46 [==============================] - 7s 142ms/step - loss: 1.3783 - lr: 0.0100\n",
      "Epoch 12/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 1.2720\n",
      "Epoch 12: loss improved from 1.37834 to 1.27201, saving model to small.h5\n",
      "46/46 [==============================] - 7s 148ms/step - loss: 1.2720 - lr: 0.0100\n",
      "Epoch 13/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 1.1389\n",
      "Epoch 13: loss improved from 1.27201 to 1.13893, saving model to small.h5\n",
      "46/46 [==============================] - 7s 147ms/step - loss: 1.1389 - lr: 0.0100\n",
      "Epoch 14/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 1.0698\n",
      "Epoch 14: loss improved from 1.13893 to 1.06976, saving model to small.h5\n",
      "46/46 [==============================] - 7s 153ms/step - loss: 1.0698 - lr: 0.0100\n",
      "Epoch 15/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.9812\n",
      "Epoch 15: loss improved from 1.06976 to 0.98120, saving model to small.h5\n",
      "46/46 [==============================] - 7s 153ms/step - loss: 0.9812 - lr: 0.0100\n",
      "Epoch 16/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.8976\n",
      "Epoch 16: loss improved from 0.98120 to 0.89758, saving model to small.h5\n",
      "46/46 [==============================] - 7s 157ms/step - loss: 0.8976 - lr: 0.0100\n",
      "Epoch 17/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.8610\n",
      "Epoch 17: loss improved from 0.89758 to 0.86103, saving model to small.h5\n",
      "46/46 [==============================] - 8s 170ms/step - loss: 0.8610 - lr: 0.0100\n",
      "Epoch 18/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.7961\n",
      "Epoch 18: loss improved from 0.86103 to 0.79614, saving model to small.h5\n",
      "46/46 [==============================] - 7s 155ms/step - loss: 0.7961 - lr: 0.0100\n",
      "Epoch 19/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.7341\n",
      "Epoch 19: loss improved from 0.79614 to 0.73405, saving model to small.h5\n",
      "46/46 [==============================] - 7s 157ms/step - loss: 0.7341 - lr: 0.0100\n",
      "Epoch 20/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.6976\n",
      "Epoch 20: loss improved from 0.73405 to 0.69757, saving model to small.h5\n",
      "46/46 [==============================] - 7s 155ms/step - loss: 0.6976 - lr: 0.0100\n",
      "Epoch 21/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.6768\n",
      "Epoch 21: loss improved from 0.69757 to 0.67684, saving model to small.h5\n",
      "46/46 [==============================] - 7s 149ms/step - loss: 0.6768 - lr: 0.0100\n",
      "Epoch 22/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.6540\n",
      "Epoch 22: loss improved from 0.67684 to 0.65402, saving model to small.h5\n",
      "46/46 [==============================] - 7s 159ms/step - loss: 0.6540 - lr: 0.0100\n",
      "Epoch 23/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.6267\n",
      "Epoch 23: loss improved from 0.65402 to 0.62669, saving model to small.h5\n",
      "46/46 [==============================] - 6s 139ms/step - loss: 0.6267 - lr: 0.0100\n",
      "Epoch 24/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.6019\n",
      "Epoch 24: loss improved from 0.62669 to 0.60189, saving model to small.h5\n",
      "46/46 [==============================] - 6s 140ms/step - loss: 0.6019 - lr: 0.0100\n",
      "Epoch 25/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.5629\n",
      "Epoch 25: loss improved from 0.60189 to 0.56285, saving model to small.h5\n",
      "46/46 [==============================] - 7s 145ms/step - loss: 0.5629 - lr: 0.0100\n",
      "Epoch 26/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.5122\n",
      "Epoch 26: loss improved from 0.56285 to 0.51223, saving model to small.h5\n",
      "46/46 [==============================] - 7s 145ms/step - loss: 0.5122 - lr: 0.0100\n",
      "Epoch 27/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.4970\n",
      "Epoch 27: loss improved from 0.51223 to 0.49700, saving model to small.h5\n",
      "46/46 [==============================] - 6s 140ms/step - loss: 0.4970 - lr: 0.0100\n",
      "Epoch 28/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.4692\n",
      "Epoch 28: loss improved from 0.49700 to 0.46921, saving model to small.h5\n",
      "46/46 [==============================] - 7s 147ms/step - loss: 0.4692 - lr: 0.0100\n",
      "Epoch 29/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.4731\n",
      "Epoch 29: loss did not improve from 0.46921\n",
      "46/46 [==============================] - 7s 147ms/step - loss: 0.4731 - lr: 0.0100\n",
      "Epoch 30/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.4599\n",
      "Epoch 30: loss improved from 0.46921 to 0.45987, saving model to small.h5\n",
      "46/46 [==============================] - 7s 149ms/step - loss: 0.4599 - lr: 0.0100\n",
      "Epoch 31/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.4477\n",
      "Epoch 31: loss improved from 0.45987 to 0.44775, saving model to small.h5\n",
      "46/46 [==============================] - 7s 144ms/step - loss: 0.4477 - lr: 0.0100\n",
      "Epoch 32/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.4511\n",
      "Epoch 32: loss did not improve from 0.44775\n",
      "46/46 [==============================] - 7s 141ms/step - loss: 0.4511 - lr: 0.0100\n",
      "Epoch 33/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.4310\n",
      "Epoch 33: loss improved from 0.44775 to 0.43099, saving model to small.h5\n",
      "46/46 [==============================] - 8s 164ms/step - loss: 0.4310 - lr: 0.0100\n",
      "Epoch 34/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.4394\n",
      "Epoch 34: loss did not improve from 0.43099\n",
      "46/46 [==============================] - 7s 152ms/step - loss: 0.4394 - lr: 0.0100\n",
      "Epoch 35/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.4445\n",
      "Epoch 35: loss did not improve from 0.43099\n",
      "46/46 [==============================] - 6s 141ms/step - loss: 0.4445 - lr: 0.0100\n",
      "Epoch 36/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.4283\n",
      "Epoch 36: loss improved from 0.43099 to 0.42829, saving model to small.h5\n",
      "46/46 [==============================] - 7s 143ms/step - loss: 0.4283 - lr: 0.0100\n",
      "Epoch 37/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.4246\n",
      "Epoch 37: loss improved from 0.42829 to 0.42459, saving model to small.h5\n",
      "46/46 [==============================] - 7s 150ms/step - loss: 0.4246 - lr: 0.0100\n",
      "Epoch 38/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.4181\n",
      "Epoch 38: loss improved from 0.42459 to 0.41815, saving model to small.h5\n",
      "46/46 [==============================] - 7s 150ms/step - loss: 0.4181 - lr: 0.0100\n",
      "Epoch 39/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.4170\n",
      "Epoch 39: loss improved from 0.41815 to 0.41697, saving model to small.h5\n",
      "46/46 [==============================] - 7s 144ms/step - loss: 0.4170 - lr: 0.0100\n",
      "Epoch 40/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.4087\n",
      "Epoch 40: loss improved from 0.41697 to 0.40874, saving model to small.h5\n",
      "46/46 [==============================] - 7s 142ms/step - loss: 0.4087 - lr: 0.0100\n",
      "Epoch 41/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.4128\n",
      "Epoch 41: loss did not improve from 0.40874\n",
      "46/46 [==============================] - 7s 147ms/step - loss: 0.4128 - lr: 0.0100\n",
      "Epoch 42/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.4193\n",
      "Epoch 42: loss did not improve from 0.40874\n",
      "46/46 [==============================] - 7s 148ms/step - loss: 0.4193 - lr: 0.0100\n",
      "Epoch 43/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.4081\n",
      "Epoch 43: loss improved from 0.40874 to 0.40813, saving model to small.h5\n",
      "46/46 [==============================] - 7s 146ms/step - loss: 0.4081 - lr: 0.0100\n",
      "Epoch 44/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.4128\n",
      "Epoch 44: loss did not improve from 0.40813\n",
      "46/46 [==============================] - 7s 141ms/step - loss: 0.4128 - lr: 0.0100\n",
      "Epoch 45/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.4109\n",
      "Epoch 45: loss did not improve from 0.40813\n",
      "46/46 [==============================] - 7s 148ms/step - loss: 0.4109 - lr: 0.0100\n",
      "Epoch 46/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.4113\n",
      "Epoch 46: loss did not improve from 0.40813\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "46/46 [==============================] - 7s 149ms/step - loss: 0.4113 - lr: 0.0100\n",
      "Epoch 47/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3585\n",
      "Epoch 47: loss improved from 0.40813 to 0.35852, saving model to small.h5\n",
      "46/46 [==============================] - 7s 147ms/step - loss: 0.3585 - lr: 0.0020\n",
      "Epoch 48/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3493\n",
      "Epoch 48: loss improved from 0.35852 to 0.34934, saving model to small.h5\n",
      "46/46 [==============================] - 7s 142ms/step - loss: 0.3493 - lr: 0.0020\n",
      "Epoch 49/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3467\n",
      "Epoch 49: loss improved from 0.34934 to 0.34670, saving model to small.h5\n",
      "46/46 [==============================] - 7s 148ms/step - loss: 0.3467 - lr: 0.0020\n",
      "Epoch 50/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3461\n",
      "Epoch 50: loss improved from 0.34670 to 0.34608, saving model to small.h5\n",
      "46/46 [==============================] - 7s 150ms/step - loss: 0.3461 - lr: 0.0020\n",
      "Epoch 51/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3450\n",
      "Epoch 51: loss improved from 0.34608 to 0.34499, saving model to small.h5\n",
      "46/46 [==============================] - 7s 149ms/step - loss: 0.3450 - lr: 0.0020\n",
      "Epoch 52/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3440\n",
      "Epoch 52: loss improved from 0.34499 to 0.34401, saving model to small.h5\n",
      "46/46 [==============================] - 7s 141ms/step - loss: 0.3440 - lr: 0.0020\n",
      "Epoch 53/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3437\n",
      "Epoch 53: loss improved from 0.34401 to 0.34372, saving model to small.h5\n",
      "46/46 [==============================] - 7s 147ms/step - loss: 0.3437 - lr: 0.0020\n",
      "Epoch 54/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3448\n",
      "Epoch 54: loss did not improve from 0.34372\n",
      "46/46 [==============================] - 7s 149ms/step - loss: 0.3448 - lr: 0.0020\n",
      "Epoch 55/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3431\n",
      "Epoch 55: loss improved from 0.34372 to 0.34314, saving model to small.h5\n",
      "46/46 [==============================] - 7s 153ms/step - loss: 0.3431 - lr: 0.0020\n",
      "Epoch 56/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3432\n",
      "Epoch 56: loss did not improve from 0.34314\n",
      "46/46 [==============================] - 6s 141ms/step - loss: 0.3432 - lr: 0.0020\n",
      "Epoch 57/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3434\n",
      "Epoch 57: loss did not improve from 0.34314\n",
      "46/46 [==============================] - 7s 143ms/step - loss: 0.3434 - lr: 0.0020\n",
      "Epoch 58/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3422\n",
      "Epoch 58: loss improved from 0.34314 to 0.34216, saving model to small.h5\n",
      "46/46 [==============================] - 7s 151ms/step - loss: 0.3422 - lr: 0.0020\n",
      "Epoch 59/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3438\n",
      "Epoch 59: loss did not improve from 0.34216\n",
      "46/46 [==============================] - 7s 150ms/step - loss: 0.3438 - lr: 0.0020\n",
      "Epoch 60/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3436\n",
      "Epoch 60: loss did not improve from 0.34216\n",
      "46/46 [==============================] - 6s 141ms/step - loss: 0.3436 - lr: 0.0020\n",
      "Epoch 61/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3431\n",
      "Epoch 61: loss did not improve from 0.34216\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "46/46 [==============================] - 7s 145ms/step - loss: 0.3431 - lr: 0.0020\n",
      "Epoch 62/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3300\n",
      "Epoch 62: loss improved from 0.34216 to 0.32998, saving model to small.h5\n",
      "46/46 [==============================] - 7s 152ms/step - loss: 0.3300 - lr: 4.0000e-04\n",
      "Epoch 63/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3291\n",
      "Epoch 63: loss improved from 0.32998 to 0.32914, saving model to small.h5\n",
      "46/46 [==============================] - 7s 152ms/step - loss: 0.3291 - lr: 4.0000e-04\n",
      "Epoch 64/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3295\n",
      "Epoch 64: loss did not improve from 0.32914\n",
      "46/46 [==============================] - 7s 146ms/step - loss: 0.3295 - lr: 4.0000e-04\n",
      "Epoch 65/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3292\n",
      "Epoch 65: loss did not improve from 0.32914\n",
      "46/46 [==============================] - 7s 144ms/step - loss: 0.3292 - lr: 4.0000e-04\n",
      "Epoch 66/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3292\n",
      "Epoch 66: loss did not improve from 0.32914\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "46/46 [==============================] - 8s 169ms/step - loss: 0.3292 - lr: 4.0000e-04\n",
      "Epoch 67/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3262\n",
      "Epoch 67: loss improved from 0.32914 to 0.32624, saving model to small.h5\n",
      "46/46 [==============================] - 7s 150ms/step - loss: 0.3262 - lr: 1.0000e-04\n",
      "Epoch 68/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3263\n",
      "Epoch 68: loss did not improve from 0.32624\n",
      "46/46 [==============================] - 8s 169ms/step - loss: 0.3263 - lr: 1.0000e-04\n",
      "Epoch 69/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3263\n",
      "Epoch 69: loss did not improve from 0.32624\n",
      "46/46 [==============================] - 7s 143ms/step - loss: 0.3263 - lr: 1.0000e-04\n",
      "Epoch 70/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3263\n",
      "Epoch 70: loss did not improve from 0.32624\n",
      "46/46 [==============================] - 7s 151ms/step - loss: 0.3263 - lr: 1.0000e-04\n",
      "Epoch 71/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3262\n",
      "Epoch 71: loss improved from 0.32624 to 0.32618, saving model to small.h5\n",
      "46/46 [==============================] - 7s 151ms/step - loss: 0.3262 - lr: 1.0000e-04\n",
      "Epoch 72/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3262\n",
      "Epoch 72: loss did not improve from 0.32618\n",
      "46/46 [==============================] - 7s 147ms/step - loss: 0.3262 - lr: 1.0000e-04\n",
      "Epoch 73/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3262\n",
      "Epoch 73: loss improved from 0.32618 to 0.32615, saving model to small.h5\n",
      "46/46 [==============================] - 7s 154ms/step - loss: 0.3262 - lr: 1.0000e-04\n",
      "Epoch 74/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3261\n",
      "Epoch 74: loss improved from 0.32615 to 0.32607, saving model to small.h5\n",
      "46/46 [==============================] - 7s 160ms/step - loss: 0.3261 - lr: 1.0000e-04\n",
      "Epoch 75/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3260\n",
      "Epoch 75: loss improved from 0.32607 to 0.32602, saving model to small.h5\n",
      "46/46 [==============================] - 7s 161ms/step - loss: 0.3260 - lr: 1.0000e-04\n",
      "Epoch 76/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3261\n",
      "Epoch 76: loss did not improve from 0.32602\n",
      "46/46 [==============================] - 7s 150ms/step - loss: 0.3261 - lr: 1.0000e-04\n",
      "Epoch 77/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3259\n",
      "Epoch 77: loss improved from 0.32602 to 0.32595, saving model to small.h5\n",
      "46/46 [==============================] - 7s 155ms/step - loss: 0.3259 - lr: 1.0000e-04\n",
      "Epoch 78/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3260\n",
      "Epoch 78: loss did not improve from 0.32595\n",
      "46/46 [==============================] - 10s 224ms/step - loss: 0.3260 - lr: 1.0000e-04\n",
      "Epoch 79/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3260\n",
      "Epoch 79: loss did not improve from 0.32595\n",
      "46/46 [==============================] - 7s 145ms/step - loss: 0.3260 - lr: 1.0000e-04\n",
      "Epoch 80/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3259\n",
      "Epoch 80: loss improved from 0.32595 to 0.32589, saving model to small.h5\n",
      "46/46 [==============================] - 8s 167ms/step - loss: 0.3259 - lr: 1.0000e-04\n",
      "Epoch 81/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3259\n",
      "Epoch 81: loss improved from 0.32589 to 0.32585, saving model to small.h5\n",
      "46/46 [==============================] - 7s 148ms/step - loss: 0.3259 - lr: 1.0000e-04\n",
      "Epoch 82/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3258\n",
      "Epoch 82: loss improved from 0.32585 to 0.32578, saving model to small.h5\n",
      "46/46 [==============================] - 7s 153ms/step - loss: 0.3258 - lr: 1.0000e-04\n",
      "Epoch 83/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3259\n",
      "Epoch 83: loss did not improve from 0.32578\n",
      "46/46 [==============================] - 6s 138ms/step - loss: 0.3259 - lr: 1.0000e-04\n",
      "Epoch 84/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3258\n",
      "Epoch 84: loss did not improve from 0.32578\n",
      "46/46 [==============================] - 7s 147ms/step - loss: 0.3258 - lr: 1.0000e-04\n",
      "Epoch 85/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3259\n",
      "Epoch 85: loss did not improve from 0.32578\n",
      "46/46 [==============================] - 7s 147ms/step - loss: 0.3259 - lr: 1.0000e-04\n",
      "Epoch 86/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3258\n",
      "Epoch 86: loss improved from 0.32578 to 0.32577, saving model to small.h5\n",
      "46/46 [==============================] - 7s 152ms/step - loss: 0.3258 - lr: 1.0000e-04\n",
      "Epoch 87/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3257\n",
      "Epoch 87: loss improved from 0.32577 to 0.32574, saving model to small.h5\n",
      "46/46 [==============================] - 7s 150ms/step - loss: 0.3257 - lr: 1.0000e-04\n",
      "Epoch 88/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3257\n",
      "Epoch 88: loss improved from 0.32574 to 0.32566, saving model to small.h5\n",
      "46/46 [==============================] - 8s 171ms/step - loss: 0.3257 - lr: 1.0000e-04\n",
      "Epoch 89/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3257\n",
      "Epoch 89: loss did not improve from 0.32566\n",
      "46/46 [==============================] - 7s 151ms/step - loss: 0.3257 - lr: 1.0000e-04\n",
      "Epoch 90/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3257\n",
      "Epoch 90: loss did not improve from 0.32566\n",
      "46/46 [==============================] - 7s 150ms/step - loss: 0.3257 - lr: 1.0000e-04\n",
      "Epoch 91/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3256\n",
      "Epoch 91: loss improved from 0.32566 to 0.32563, saving model to small.h5\n",
      "46/46 [==============================] - 7s 145ms/step - loss: 0.3256 - lr: 1.0000e-04\n",
      "Epoch 92/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3257\n",
      "Epoch 92: loss did not improve from 0.32563\n",
      "46/46 [==============================] - 7s 148ms/step - loss: 0.3257 - lr: 1.0000e-04\n",
      "Epoch 93/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3256\n",
      "Epoch 93: loss improved from 0.32563 to 0.32560, saving model to small.h5\n",
      "46/46 [==============================] - 7s 152ms/step - loss: 0.3256 - lr: 1.0000e-04\n",
      "Epoch 94/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3256\n",
      "Epoch 94: loss improved from 0.32560 to 0.32559, saving model to small.h5\n",
      "46/46 [==============================] - 7s 151ms/step - loss: 0.3256 - lr: 1.0000e-04\n",
      "Epoch 95/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3255\n",
      "Epoch 95: loss improved from 0.32559 to 0.32546, saving model to small.h5\n",
      "46/46 [==============================] - 7s 145ms/step - loss: 0.3255 - lr: 1.0000e-04\n",
      "Epoch 96/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3255\n",
      "Epoch 96: loss did not improve from 0.32546\n",
      "46/46 [==============================] - 7s 143ms/step - loss: 0.3255 - lr: 1.0000e-04\n",
      "Epoch 97/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3255\n",
      "Epoch 97: loss did not improve from 0.32546\n",
      "46/46 [==============================] - 7s 151ms/step - loss: 0.3255 - lr: 1.0000e-04\n",
      "Epoch 98/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3255\n",
      "Epoch 98: loss did not improve from 0.32546\n",
      "46/46 [==============================] - 7s 150ms/step - loss: 0.3255 - lr: 1.0000e-04\n",
      "Epoch 99/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3255\n",
      "Epoch 99: loss did not improve from 0.32546\n",
      "46/46 [==============================] - 7s 144ms/step - loss: 0.3255 - lr: 1.0000e-04\n",
      "Epoch 100/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3254\n",
      "Epoch 100: loss improved from 0.32546 to 0.32542, saving model to small.h5\n",
      "46/46 [==============================] - 7s 150ms/step - loss: 0.3254 - lr: 1.0000e-04\n",
      "Epoch 101/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3254\n",
      "Epoch 101: loss improved from 0.32542 to 0.32539, saving model to small.h5\n",
      "46/46 [==============================] - 7s 159ms/step - loss: 0.3254 - lr: 1.0000e-04\n",
      "Epoch 102/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3254\n",
      "Epoch 102: loss did not improve from 0.32539\n",
      "46/46 [==============================] - 7s 150ms/step - loss: 0.3254 - lr: 1.0000e-04\n",
      "Epoch 103/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3254\n",
      "Epoch 103: loss did not improve from 0.32539\n",
      "46/46 [==============================] - 7s 147ms/step - loss: 0.3254 - lr: 1.0000e-04\n",
      "Epoch 104/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3253\n",
      "Epoch 104: loss improved from 0.32539 to 0.32528, saving model to small.h5\n",
      "46/46 [==============================] - 7s 143ms/step - loss: 0.3253 - lr: 1.0000e-04\n",
      "Epoch 105/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3253\n",
      "Epoch 105: loss did not improve from 0.32528\n",
      "46/46 [==============================] - 7s 147ms/step - loss: 0.3253 - lr: 1.0000e-04\n",
      "Epoch 106/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3253\n",
      "Epoch 106: loss improved from 0.32528 to 0.32526, saving model to small.h5\n",
      "46/46 [==============================] - 7s 152ms/step - loss: 0.3253 - lr: 1.0000e-04\n",
      "Epoch 107/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3253\n",
      "Epoch 107: loss did not improve from 0.32526\n",
      "46/46 [==============================] - 7s 151ms/step - loss: 0.3253 - lr: 1.0000e-04\n",
      "Epoch 108/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3253\n",
      "Epoch 108: loss improved from 0.32526 to 0.32525, saving model to small.h5\n",
      "46/46 [==============================] - 7s 146ms/step - loss: 0.3253 - lr: 1.0000e-04\n",
      "Epoch 109/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3252\n",
      "Epoch 109: loss improved from 0.32525 to 0.32522, saving model to small.h5\n",
      "46/46 [==============================] - 7s 148ms/step - loss: 0.3252 - lr: 1.0000e-04\n",
      "Epoch 110/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3252\n",
      "Epoch 110: loss improved from 0.32522 to 0.32516, saving model to small.h5\n",
      "46/46 [==============================] - 7s 152ms/step - loss: 0.3252 - lr: 1.0000e-04\n",
      "Epoch 111/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3252\n",
      "Epoch 111: loss did not improve from 0.32516\n",
      "46/46 [==============================] - 8s 173ms/step - loss: 0.3252 - lr: 1.0000e-04\n",
      "Epoch 112/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3251\n",
      "Epoch 112: loss improved from 0.32516 to 0.32515, saving model to small.h5\n",
      "46/46 [==============================] - 8s 173ms/step - loss: 0.3251 - lr: 1.0000e-04\n",
      "Epoch 113/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3252\n",
      "Epoch 113: loss did not improve from 0.32515\n",
      "46/46 [==============================] - 9s 199ms/step - loss: 0.3252 - lr: 1.0000e-04\n",
      "Epoch 114/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3250\n",
      "Epoch 114: loss improved from 0.32515 to 0.32504, saving model to small.h5\n",
      "46/46 [==============================] - 11s 245ms/step - loss: 0.3250 - lr: 1.0000e-04\n",
      "Epoch 115/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3252\n",
      "Epoch 115: loss did not improve from 0.32504\n",
      "46/46 [==============================] - 9s 199ms/step - loss: 0.3252 - lr: 1.0000e-04\n",
      "Epoch 116/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3251\n",
      "Epoch 116: loss did not improve from 0.32504\n",
      "46/46 [==============================] - 10s 218ms/step - loss: 0.3251 - lr: 1.0000e-04\n",
      "Epoch 117/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3250\n",
      "Epoch 117: loss improved from 0.32504 to 0.32499, saving model to small.h5\n",
      "46/46 [==============================] - 9s 202ms/step - loss: 0.3250 - lr: 1.0000e-04\n",
      "Epoch 118/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3250\n",
      "Epoch 118: loss improved from 0.32499 to 0.32497, saving model to small.h5\n",
      "46/46 [==============================] - 10s 208ms/step - loss: 0.3250 - lr: 1.0000e-04\n",
      "Epoch 119/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3249\n",
      "Epoch 119: loss improved from 0.32497 to 0.32488, saving model to small.h5\n",
      "46/46 [==============================] - 10s 214ms/step - loss: 0.3249 - lr: 1.0000e-04\n",
      "Epoch 120/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3250\n",
      "Epoch 120: loss did not improve from 0.32488\n",
      "46/46 [==============================] - 10s 208ms/step - loss: 0.3250 - lr: 1.0000e-04\n",
      "Epoch 121/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3250\n",
      "Epoch 121: loss did not improve from 0.32488\n",
      "46/46 [==============================] - 9s 193ms/step - loss: 0.3250 - lr: 1.0000e-04\n",
      "Epoch 122/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3249\n",
      "Epoch 122: loss did not improve from 0.32488\n",
      "46/46 [==============================] - 10s 219ms/step - loss: 0.3249 - lr: 1.0000e-04\n",
      "Epoch 123/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3250\n",
      "Epoch 123: loss did not improve from 0.32488\n",
      "46/46 [==============================] - 9s 201ms/step - loss: 0.3250 - lr: 1.0000e-04\n",
      "Epoch 124/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3250\n",
      "Epoch 124: loss did not improve from 0.32488\n",
      "46/46 [==============================] - 11s 236ms/step - loss: 0.3250 - lr: 1.0000e-04\n",
      "Epoch 125/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3250\n",
      "Epoch 125: loss did not improve from 0.32488\n",
      "46/46 [==============================] - 11s 234ms/step - loss: 0.3250 - lr: 1.0000e-04\n",
      "Epoch 126/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3250\n",
      "Epoch 126: loss did not improve from 0.32488\n",
      "46/46 [==============================] - 11s 241ms/step - loss: 0.3250 - lr: 1.0000e-04\n",
      "Epoch 127/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3249\n",
      "Epoch 127: loss improved from 0.32488 to 0.32486, saving model to small.h5\n",
      "46/46 [==============================] - 12s 253ms/step - loss: 0.3249 - lr: 1.0000e-04\n",
      "Epoch 128/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3248\n",
      "Epoch 128: loss improved from 0.32486 to 0.32482, saving model to small.h5\n",
      "46/46 [==============================] - 11s 242ms/step - loss: 0.3248 - lr: 1.0000e-04\n",
      "Epoch 129/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3249\n",
      "Epoch 129: loss did not improve from 0.32482\n",
      "46/46 [==============================] - 12s 250ms/step - loss: 0.3249 - lr: 1.0000e-04\n",
      "Epoch 130/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3249\n",
      "Epoch 130: loss did not improve from 0.32482\n",
      "46/46 [==============================] - 12s 254ms/step - loss: 0.3249 - lr: 1.0000e-04\n",
      "Epoch 131/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3247\n",
      "Epoch 131: loss improved from 0.32482 to 0.32467, saving model to small.h5\n",
      "46/46 [==============================] - 12s 258ms/step - loss: 0.3247 - lr: 1.0000e-04\n",
      "Epoch 132/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3248\n",
      "Epoch 132: loss did not improve from 0.32467\n",
      "46/46 [==============================] - 11s 227ms/step - loss: 0.3248 - lr: 1.0000e-04\n",
      "Epoch 133/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3248\n",
      "Epoch 133: loss did not improve from 0.32467\n",
      "46/46 [==============================] - 14s 307ms/step - loss: 0.3248 - lr: 1.0000e-04\n",
      "Epoch 134/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3247\n",
      "Epoch 134: loss did not improve from 0.32467\n",
      "46/46 [==============================] - 10s 216ms/step - loss: 0.3247 - lr: 1.0000e-04\n",
      "Epoch 135/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3247\n",
      "Epoch 135: loss did not improve from 0.32467\n",
      "46/46 [==============================] - 7s 160ms/step - loss: 0.3247 - lr: 1.0000e-04\n",
      "Epoch 136/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3246\n",
      "Epoch 136: loss improved from 0.32467 to 0.32464, saving model to small.h5\n",
      "46/46 [==============================] - 7s 155ms/step - loss: 0.3246 - lr: 1.0000e-04\n",
      "Epoch 137/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3247\n",
      "Epoch 137: loss did not improve from 0.32464\n",
      "46/46 [==============================] - 8s 179ms/step - loss: 0.3247 - lr: 1.0000e-04\n",
      "Epoch 138/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3247\n",
      "Epoch 138: loss did not improve from 0.32464\n",
      "46/46 [==============================] - 8s 174ms/step - loss: 0.3247 - lr: 1.0000e-04\n",
      "Epoch 139/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3247\n",
      "Epoch 139: loss did not improve from 0.32464\n",
      "46/46 [==============================] - 8s 183ms/step - loss: 0.3247 - lr: 1.0000e-04\n",
      "Epoch 140/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3246\n",
      "Epoch 140: loss improved from 0.32464 to 0.32460, saving model to small.h5\n",
      "46/46 [==============================] - 8s 174ms/step - loss: 0.3246 - lr: 1.0000e-04\n",
      "Epoch 141/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3246\n",
      "Epoch 141: loss improved from 0.32460 to 0.32458, saving model to small.h5\n",
      "46/46 [==============================] - 7s 153ms/step - loss: 0.3246 - lr: 1.0000e-04\n",
      "Epoch 142/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3246\n",
      "Epoch 142: loss improved from 0.32458 to 0.32456, saving model to small.h5\n",
      "46/46 [==============================] - 7s 161ms/step - loss: 0.3246 - lr: 1.0000e-04\n",
      "Epoch 143/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3246\n",
      "Epoch 143: loss did not improve from 0.32456\n",
      "46/46 [==============================] - 9s 206ms/step - loss: 0.3246 - lr: 1.0000e-04\n",
      "Epoch 144/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3246\n",
      "Epoch 144: loss did not improve from 0.32456\n",
      "46/46 [==============================] - 9s 181ms/step - loss: 0.3246 - lr: 1.0000e-04\n",
      "Epoch 145/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3246\n",
      "Epoch 145: loss did not improve from 0.32456\n",
      "46/46 [==============================] - 8s 174ms/step - loss: 0.3246 - lr: 1.0000e-04\n",
      "Epoch 146/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3246\n",
      "Epoch 146: loss did not improve from 0.32456\n",
      "46/46 [==============================] - 8s 185ms/step - loss: 0.3246 - lr: 1.0000e-04\n",
      "Epoch 147/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3246\n",
      "Epoch 147: loss did not improve from 0.32456\n",
      "46/46 [==============================] - 8s 173ms/step - loss: 0.3246 - lr: 1.0000e-04\n",
      "Epoch 148/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3246\n",
      "Epoch 148: loss improved from 0.32456 to 0.32455, saving model to small.h5\n",
      "46/46 [==============================] - 8s 182ms/step - loss: 0.3246 - lr: 1.0000e-04\n",
      "Epoch 149/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3245\n",
      "Epoch 149: loss improved from 0.32455 to 0.32448, saving model to small.h5\n",
      "46/46 [==============================] - 8s 182ms/step - loss: 0.3245 - lr: 1.0000e-04\n",
      "Epoch 150/150\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3245\n",
      "Epoch 150: loss did not improve from 0.32448\n",
      "46/46 [==============================] - 9s 186ms/step - loss: 0.3245 - lr: 1.0000e-04\n",
      "top 1 accuracy: 0.1835883171070932\n",
      "top 3 accuracy: 0.627260083449235\n",
      "ndcg: 0.4391126626018354\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name in size:\n",
    "    print(\"size: \",name)\n",
    "    data = pd.read_csv(f'code_completion_lib/methods/models/small/data.csv')\n",
    "    X = data[[\"varible_name\"]]\n",
    "    for element in X.values:\n",
    "        element[0]=element[0].replace(\".\", \" \").replace(\"_\", \" \").replace(\",\", \" \").replace(\"[\", \" \").replace(\"]\", \" \").replace(\"'\", \" \").replace('\"', ' ')\n",
    "    y = data[[\"method\"]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    s = pd.Series(data=X_train[\"varible_name\"]+\" \" + y_train[\"method\"])\n",
    "\n",
    "    tokenizer = Tokenizer(oov_token='<oov>',split=\" \", filters='!',lower=False) # For those words which are not found in word_index\n",
    "    tokenizer.fit_on_texts(s)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    #print(\"Total number of words: \", total_words)\n",
    "\n",
    "\n",
    "    input_sequences = []\n",
    "    for line in s:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        input_sequences.append(token_list)\n",
    "\n",
    "    #print(\"Total input sequences: \", len(input_sequences))\n",
    "    # pad sequences\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "    # create features and label\n",
    "    X = input_sequences[:,:-1]\n",
    "    labels = input_sequences[:,-1]\n",
    "    y = to_categorical(labels, num_classes=total_words)\n",
    "\n",
    "    # saving the tokenizer for predict function.\n",
    "    pickle.dump(tokenizer, open(f'{name}.pkl', 'wb'))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 200, input_length=max_sequence_len - 1))\n",
    "    model.add(LSTM(500, return_sequences=True))\n",
    "    model.add(LSTM(500))\n",
    "    model.add(Dense(500, activation=\"relu\"))\n",
    "    model.add(Dense(total_words, activation=\"softmax\"))\n",
    "    print(model.summary())\n",
    "\n",
    "    checkpoint = ModelCheckpoint(f\"{name}.h5\", monitor='loss', verbose=1,\n",
    "                                 save_best_only=True, mode='auto')\n",
    "\n",
    "    reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "    logdir='logsnextword1'\n",
    "    tensorboard_Visualization = TensorBoard(log_dir=logdir)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.01))\n",
    "\n",
    "    model.fit(X, y, epochs=150, callbacks=[checkpoint, reduce, tensorboard_Visualization])\n",
    "\n",
    "\n",
    "    model = load_model(f'{name}.h5')\n",
    "    tokenizer = pickle.load(open(f'{name}.pkl', 'rb'))\n",
    "    y_pred_top1 = []\n",
    "    y_pred_top3 = []\n",
    "    y_true = y_test['method'].values.tolist()\n",
    "    x_for_pred = X_test['varible_name'].values.tolist()\n",
    "    for i in range(X_test.shape[0]):\n",
    "        token_list = tokenizer.texts_to_sequences([x_for_pred[i]])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predict_x=model.predict(token_list,verbose=0)\n",
    "        f = predict_x[0].copy()\n",
    "        f.sort()\n",
    "        a = np.flip(f)\n",
    "        max_index = []\n",
    "        for i in range(len(predict_x[0])):\n",
    "            if predict_x[0][i]== a[0] or predict_x[0][i]== a[1] or predict_x[0][i]== a[2]:\n",
    "                max_index.append(i)\n",
    "        classes_x=np.argmax(predict_x,axis=1)\n",
    "        max_index = max_index[:3]\n",
    "        result = []\n",
    "        for j in range(3):\n",
    "            for word, index in tokenizer.word_index.items():\n",
    "                if index == max_index[j]:\n",
    "                    result.append(word)\n",
    "                    break\n",
    "        y_pred_top1.append(result[0])\n",
    "        y_pred_top3.append(result)\n",
    "\n",
    "    acc_top1 = accuracy_score(y_true, y_pred_top1)\n",
    "    acc_top3 = top_3(y_true, y_pred_top3)\n",
    "    ndcg = nDCG(y_true, y_pred_top3)\n",
    "\n",
    "    print(f\"top 1 accuracy: {acc_top1}\")\n",
    "    print(f\"top 3 accuracy: {acc_top3}\")\n",
    "    print(f\"ndcg: {ndcg}\\n\")\n",
    "    print(\"-----------------------------------------------------------------------------------------------------------\")\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
